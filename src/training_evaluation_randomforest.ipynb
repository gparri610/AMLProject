{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluation for Random Forest Model\n",
    "This model trains differently than the neural network based models which is why there is a seperate notebook. Training is computationally very expensive which is why we were only able to train on a fraction of the training data used for the other models (6%). Training still takes roughly 1 hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from data_handling import complete_data_preparation\n",
    "from benchmarks import RandomForest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading all the data\n",
    "batches_train, batches_val, batches_test = complete_data_preparation(sequence_length=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only selecting fraction of the data due to computational cost\n",
    "batches_train = batches_train[:1000]\n",
    "batches_val = batches_val[:1000]\n",
    "batches_test = batches_test[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Target Vector: (64000,)\n"
     ]
    }
   ],
   "source": [
    "# Reshape targets of each batch to be have shape S x 1 (S = Number of Samples) and aggregating them to one vector\n",
    "reshaped_batches_target = [batch[:,-1,-1].view(batch.shape[0],1) for batch in batches_train]\n",
    "stacked_targets = torch.cat(reshaped_batches_target, dim=0).numpy().ravel()\n",
    "print(f'Shape of Target Vector: {stacked_targets.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Feature Matrix: (64000, 195)\n"
     ]
    }
   ],
   "source": [
    "# Reshape features of each batch to be have shape S x 1 (S = Number of Samples) and aggregating them to one matrix\n",
    "reshaped_batches = [batch[:,:,:-1].reshape(batch.shape[0], -1) for batch in batches_train]\n",
    "stacked_data = torch.cat(reshaped_batches, dim=0).numpy()\n",
    "print(f'Shape of Feature Matrix: {stacked_data.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the Model\n",
    "model = RandomForest(n_estimators=100, max_depth=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainign the model\n",
    "sse_train_unadjusted = model.train(stacked_data, stacked_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the other models we calculated the mean squared error for the batch and summed over all the batches. For this model we no longer have batches so we adjusted the loss by dividing by 64 to account for this. Additionally, as we are only using 1000 of the 16670 batches for the data set so we adjust by multiplying with $\\frac{16670}{1000}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Test Loss: 7.0597633312831505\n"
     ]
    }
   ],
   "source": [
    "sse_train_adjusted = sse_train_unadjusted / 64 * (16670/1000)\n",
    "print(f'Estimated Train Loss: {sse_train_adjusted}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_saving_path = \"modelDumps/\" + \"RandomForest\" + \".pt\"\n",
    "import os\n",
    "if not os.path.exists('modelDumps'):\n",
    "    os.makedirs('modelDumps')\n",
    "torch.save(model, model_saving_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Target Vector: (64000,)\n",
      "Shape of Feature Matrix: (64000, 195)\n"
     ]
    }
   ],
   "source": [
    "# Preparing the test data\n",
    "\n",
    "# Reshape targets of each batch to be have shape S x 1 (S = Number of Samples) and aggregating them to one vector\n",
    "reshaped_batches_target = [batch[:,-1,-1].view(batch.shape[0],1) for batch in batches_test]\n",
    "stacked_targets_test = torch.cat(reshaped_batches_target, dim=0).numpy().ravel()\n",
    "print(f'Shape of Target Vector: {stacked_targets.shape}')\n",
    "\n",
    "# Reshape features of each batch to be have shape S x 1 (S = Number of Samples) and aggregating them to one matrix\n",
    "reshaped_batches = [batch[:,:,:-1].reshape(batch.shape[0], -1) for batch in batches_train]\n",
    "stacked_data_test = torch.cat(reshaped_batches, dim=0).numpy()\n",
    "print(f'Shape of Feature Matrix: {stacked_data.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the predicitons of the model\n",
    "predictions = model(stacked_data_test)\n",
    "sse_test_unadjusted = np.sum((predictions - stacked_targets_test)**2)\n",
    "adjusted_test_loss = sse_test_unadjusted / 64 * (3880/1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we have to adjust for the fact that we summed summed the square loss and not the averaque square loss of the batches so we adjusted the loss by dividing by 64 to account for this. Additionally, as we are only using 1000 of the 3880 batches for the data set so we adjust by multiplying with $\\frac{3880}{1000}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Test Loss: 5.415680542938186\n"
     ]
    }
   ],
   "source": [
    "print(f'Estimated Test Loss: {adjusted_test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trees:  100\n",
      "Tree depths:  [30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30]\n",
      "Number of leaf nodes per tree:  [8430, 8008, 7450, 7836, 6907, 6237, 6630, 7679, 7921, 6977, 6908, 6294, 4996, 5234, 6063, 7568, 6409, 5964, 8133, 8107, 5926, 6724, 5491, 8287, 6578, 6558, 8393, 7998, 7279, 7080, 6282, 8439, 6311, 8844, 6967, 8507, 6542, 8889, 7198, 7922, 6194, 5516, 6834, 5435, 5841, 6666, 6244, 5830, 6589, 6643, 6316, 6355, 5489, 7468, 7800, 7436, 7424, 9542, 6708, 6282, 6560, 6752, 7102, 7447, 7334, 7820, 10174, 5951, 6985, 5838, 6218, 5605, 9692, 6661, 6984, 8646, 6589, 5813, 6832, 7951, 8165, 7320, 7521, 8140, 5249, 7486, 6514, 7000, 5749, 7933, 6997, 6269, 7239, 5476, 7561, 7520, 7042, 8755, 5436, 6424]\n",
      "Total number of leaf nodes: 701328\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "# Number of trees\n",
    "print(\"Number of trees: \", len(model.model.estimators_))\n",
    "\n",
    "# Depth of each tree\n",
    "tree_depths = [tree.tree_.max_depth for tree in model.model.estimators_]\n",
    "print(\"Tree depths: \", tree_depths)\n",
    "\n",
    "# Number of leaf nodes in each tree\n",
    "leaf_counts = [tree.tree_.n_leaves for tree in model.model.estimators_]\n",
    "print(\"Number of leaf nodes per tree: \", leaf_counts)\n",
    "\n",
    "print(f\"Total number of leaf nodes: {np.sum(leaf_counts)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
